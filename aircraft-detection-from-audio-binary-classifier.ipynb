{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7077573,"sourceType":"datasetVersion","datasetId":4076770}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Acoustic Aircraft Detection (Binary Classification)\n\nThis notebook demonstrates how to train a simple binary classifier for detecting aircraft with the **AeroSonicDB (YPAD-0523)** dataset of low-flying aircraft sounds. We briefly introduce the dataset, then build a pre-processing pipeline to extract spectrograms from the audio, then train a binary CNN classifier and evaluate its performance.","metadata":{}},{"cell_type":"markdown","source":"## Import Dependencies","metadata":{}},{"cell_type":"code","source":"# import required libraries\nimport os\nimport math\nimport random\nimport pandas as pd\nimport numpy as np\nimport IPython.display as ipd\nimport librosa\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout, BatchNormalization, SpatialDropout2D\nfrom tensorflow.keras import regularizers\nfrom sklearn.metrics import average_precision_score, PrecisionRecallDisplay\nfrom sklearn.utils import class_weight\ntf.get_logger().setLevel('ERROR')","metadata":{"execution":{"iopub.status.busy":"2025-09-01T18:13:50.337186Z","iopub.execute_input":"2025-09-01T18:13:50.337467Z","iopub.status.idle":"2025-09-01T18:14:09.134177Z","shell.execute_reply.started":"2025-09-01T18:13:50.337436Z","shell.execute_reply":"2025-09-01T18:14:09.133162Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load dataset and check directory structure\n\nAeroSonicDB (YPAD-0523) contains two distinct audio datasets. \n\nThe first resides in the \"audio/audio\" directory, and contains 1,895 audio files split between \"positive\" (aircraft) and \"negative\" (silence/no aircraft) classes. This dataset is supplemented by the \"sample_meta.csv\" file, which provides additional information about each of the audio samples. Model training, validation and testing is performed entirely on this dataset (for this particular notebook).\n\nThe second dataset can be found in the \"env_audio/env_audio\" directory and contains six, one-hour long \"environmental recordings\". These recordings are accompanied by the \"environment_class_mappings.csv\" file, which maps aircraft audio events to a timestamp in the recording. This dataset of continuous audio with annotations provides an ancilliary method for evaluating model performance on a stream of \"real-world\" and \"real-time\" data. \n","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/audio-dataset-of-low-flying-aircraft-aerosonicdb'\n\n# take a look at the directory files and structure\nprint(os.listdir(DATA_DIR))\nprint(os.listdir(DATA_DIR + '/audio'))\nprint(os.listdir(DATA_DIR + '/audio/audio'))\nprint(os.listdir(DATA_DIR + '/env_audio'))\nprint(os.listdir(DATA_DIR + '/env_audio/env_audio'))","metadata":{"execution":{"iopub.status.busy":"2025-09-01T18:14:15.186105Z","iopub.execute_input":"2025-09-01T18:14:15.187057Z","iopub.status.idle":"2025-09-01T18:14:15.239357Z","shell.execute_reply.started":"2025-09-01T18:14:15.187006Z","shell.execute_reply":"2025-09-01T18:14:15.238237Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['env_audio', 'audio', 'sample_meta.csv', 'environment_class_mappings.csv', 'environment_mappings_raw.csv']\n['audio']\n['0', '1']\n['env_audio']\n['1_AUDIO.wav', '4_AUDIO.wav', '2_AUDIO.wav', '6_AUDIO.wav', '3_AUDIO.wav', '5_AUDIO.wav']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# set a path to the audio/audio directory\nAUDIO_DIR = os.path.join(DATA_DIR, 'audio/audio')\n\n# set a path to the env_audio/env_audio directory\nENV_DIR = os.path.join(DATA_DIR, 'env_audio/env_audio')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:54.635471Z","iopub.execute_input":"2023-12-01T06:19:54.635722Z","iopub.status.idle":"2023-12-01T06:19:54.640234Z","shell.execute_reply.started":"2023-12-01T06:19:54.6357Z","shell.execute_reply":"2023-12-01T06:19:54.639283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verify the dataset directory and \"sample_meta.csv\" contain the same number of files","metadata":{}},{"cell_type":"code","source":"# take a look at the audio directory, \n# how many negative class \"0\", how many positive \"1\"?\nprint(os.listdir(AUDIO_DIR))\n\nfor i in ['0', '1']:\n    dir_files = len(os.listdir(os.path.join(AUDIO_DIR, i)))\n    print(f'Class {i} contains {dir_files} samples')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:54.642347Z","iopub.execute_input":"2023-12-01T06:19:54.642616Z","iopub.status.idle":"2023-12-01T06:19:55.404691Z","shell.execute_reply.started":"2023-12-01T06:19:54.642594Z","shell.execute_reply":"2023-12-01T06:19:55.403852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the sample_meta.csv file for a look\ndf = pd.read_csv(os.path.join(DATA_DIR, 'sample_meta.csv'))\n# sanity check on the number of samples in each class\ndf['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.405776Z","iopub.execute_input":"2023-12-01T06:19:55.406071Z","iopub.status.idle":"2023-12-01T06:19:55.473563Z","shell.execute_reply.started":"2023-12-01T06:19:55.406047Z","shell.execute_reply":"2023-12-01T06:19:55.472663Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# take a look at all of the columns/labels available for each sample\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.474914Z","iopub.execute_input":"2023-12-01T06:19:55.475233Z","iopub.status.idle":"2023-12-01T06:19:55.482583Z","shell.execute_reply.started":"2023-12-01T06:19:55.475202Z","shell.execute_reply":"2023-12-01T06:19:55.481821Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fetch a random file from each class","metadata":{}},{"cell_type":"code","source":"## Fetch a random file from each class\nrandom.seed(42)\nNEG_FILE = random.sample(os.listdir(os.path.join(AUDIO_DIR, '0')), 1)[0]\nPOS_FILE = random.sample(os.listdir(os.path.join(AUDIO_DIR, '1')), 1)[0]\nprint(NEG_FILE)\nprint(POS_FILE)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.483833Z","iopub.execute_input":"2023-12-01T06:19:55.484157Z","iopub.status.idle":"2023-12-01T06:19:55.49461Z","shell.execute_reply.started":"2023-12-01T06:19:55.484133Z","shell.execute_reply":"2023-12-01T06:19:55.493756Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build helper functions","metadata":{}},{"cell_type":"code","source":"# define a function to build a filepath from a filename and class combination\ndef get_audio_path(df, filename):\n    # locate the filename and fetch the corresponding class (\"fclass\" == file class)\n    fclass = df.loc[df['filename'] == filename, 'class'].values[0]\n    filepath = os.path.join(AUDIO_DIR, str(fclass), filename)\n    return filepath, fclass","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.495753Z","iopub.execute_input":"2023-12-01T06:19:55.496054Z","iopub.status.idle":"2023-12-01T06:19:55.501555Z","shell.execute_reply.started":"2023-12-01T06:19:55.49603Z","shell.execute_reply":"2023-12-01T06:19:55.500286Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the function above works with our example files\nprint(get_audio_path(df=df, filename=POS_FILE))\nprint(get_audio_path(df=df, filename=NEG_FILE))","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.502987Z","iopub.execute_input":"2023-12-01T06:19:55.503267Z","iopub.status.idle":"2023-12-01T06:19:55.518601Z","shell.execute_reply.started":"2023-12-01T06:19:55.503244Z","shell.execute_reply":"2023-12-01T06:19:55.517695Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to load a file to play and show it's waveform\ndef load_show_audio(filename):\n    path, fclass = get_audio_path(df=df, filename=filename)\n    signal, sr = librosa.load(path)\n    print(f'{filename} sample rate: {str(sr)}')\n    plt.figure(figsize=(6, 3))\n    librosa.display.waveshow(y=signal, sr=sr)\n    plt.show()\n    return ipd.Audio(path)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.522333Z","iopub.execute_input":"2023-12-01T06:19:55.522693Z","iopub.status.idle":"2023-12-01T06:19:55.531342Z","shell.execute_reply.started":"2023-12-01T06:19:55.522667Z","shell.execute_reply":"2023-12-01T06:19:55.530418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load and play the positive/aircraft example\nload_show_audio(filename=POS_FILE)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:19:55.532569Z","iopub.execute_input":"2023-12-01T06:19:55.53293Z","iopub.status.idle":"2023-12-01T06:20:06.262747Z","shell.execute_reply.started":"2023-12-01T06:19:55.532878Z","shell.execute_reply":"2023-12-01T06:20:06.261557Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load and play the negative/silence example\nload_show_audio(filename=NEG_FILE)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:06.264031Z","iopub.execute_input":"2023-12-01T06:20:06.264533Z","iopub.status.idle":"2023-12-01T06:20:06.718664Z","shell.execute_reply.started":"2023-12-01T06:20:06.264505Z","shell.execute_reply":"2023-12-01T06:20:06.717689Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Audio preprocessing and feature extraction","metadata":{}},{"cell_type":"code","source":"# set some constants for feature extraction, training and inference\nSR = 22050 # sample rate of the audio files\nDURATION = 5 # length of a segment in seconds\nSAMPLES_PER_SEGMENT = SR*DURATION # the number of samples per segment we expect\nN_FFT = 2048 # approx frequency resolution of 21.5 Hz\nHOP_LENGTH = 1024 \nEXP_VECTORS_PER_SEGMENT = math.floor(SAMPLES_PER_SEGMENT/HOP_LENGTH)\nN_MELS = 128 # the number of frequency bins for spectrogram\nEXP_INPUT_SHAPE = (N_MELS, EXP_VECTORS_PER_SEGMENT) # the expected shape of the spectrogram\nprint('Expected spectrogram shape:', EXP_INPUT_SHAPE)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:06.719966Z","iopub.execute_input":"2023-12-01T06:20:06.720251Z","iopub.status.idle":"2023-12-01T06:20:06.726084Z","shell.execute_reply.started":"2023-12-01T06:20:06.720227Z","shell.execute_reply":"2023-12-01T06:20:06.725164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to load a file and chop it into spectrograms equal to the segment length\ndef audio_to_spectrogram(filename):\n    path, fclass = get_audio_path(df=df, filename=filename)\n    signal, sr = librosa.load(path)\n\n    \n    if sr != SR:\n        raise ValueError('Sample rate mismatch between audio and target')\n        \n    clip_segments = math.ceil(len(signal) / SAMPLES_PER_SEGMENT)\n    \n    # empty list to hold the spectrograms for this clip\n    specs = []\n    \n    for segment in range(clip_segments):\n        \n        start = SAMPLES_PER_SEGMENT * segment\n        end = start + SAMPLES_PER_SEGMENT - HOP_LENGTH\n        \n        spec = librosa.feature.melspectrogram(y=signal[start:end], \n                                              sr=sr, n_fft=N_FFT, \n                                              n_mels=N_MELS, \n                                              hop_length=HOP_LENGTH,\n                                              window='hann')\n        \n        db_spec = librosa.power_to_db(spec, ref=0.0)\n        \n        if db_spec.shape[1] == EXP_VECTORS_PER_SEGMENT:\n            specs.append(db_spec)\n        \n        # if the clip is shorter than the segment, add zero padding to the right\n        elif db_spec.shape[1] < EXP_VECTORS_PER_SEGMENT:\n            n_short = EXP_VECTORS_PER_SEGMENT - db_spec.shape[1]\n            db_spec = np.pad(db_spec, [(0, 0), (0, n_short)], 'constant')\n            specs.append(db_spec)\n        \n    return (specs, fclass)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:06.727436Z","iopub.execute_input":"2023-12-01T06:20:06.727921Z","iopub.status.idle":"2023-12-01T06:20:06.73984Z","shell.execute_reply.started":"2023-12-01T06:20:06.727862Z","shell.execute_reply":"2023-12-01T06:20:06.738925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# double check the segmentation, spectrogram and padding are working correctly on a single file\nspecs, fclass = audio_to_spectrogram(POS_FILE)\n\nfig, axes = plt.subplots(1,len(specs), sharey='row', figsize=(11, 3))\n\ncount = 0\n\nfor spec in specs:\n    axes[count] = librosa.display.specshow(spec, ax=axes[count])\n    count += 1\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:06.741357Z","iopub.execute_input":"2023-12-01T06:20:06.741633Z","iopub.status.idle":"2023-12-01T06:20:08.349797Z","shell.execute_reply.started":"2023-12-01T06:20:06.74161Z","shell.execute_reply":"2023-12-01T06:20:08.348816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to apply min-max scaling to squeeze spectrogram values between 0 and 1\ndef normalise_array(array):\n    array = np.asarray(array)\n    min_val = array.min()\n    max_val = array.max()\n    \n    norm_array = (array - min_val) / (max_val - min_val)\n    \n    return norm_array","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:08.351243Z","iopub.execute_input":"2023-12-01T06:20:08.351844Z","iopub.status.idle":"2023-12-01T06:20:08.357612Z","shell.execute_reply.started":"2023-12-01T06:20:08.351808Z","shell.execute_reply":"2023-12-01T06:20:08.356643Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# wrapper function to take a list of files and extract their features \n# -> array of features (X) and array of corresponding labels (y)\ndef preprocess(file_list):\n    \n    data = {'feature': [], 'label': []}\n    \n    for file in file_list:\n        specs, fclass = audio_to_spectrogram(filename=file)\n        \n        for spec in specs:\n            norm_spec = normalise_array(spec)\n            data['feature'].append(norm_spec)\n            data['label'].append(fclass)\n    \n    X = np.asarray(data['feature'])\n    y = np.asarray(data['label'])\n    \n    return X, y","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:08.359049Z","iopub.execute_input":"2023-12-01T06:20:08.360186Z","iopub.status.idle":"2023-12-01T06:20:08.374942Z","shell.execute_reply.started":"2023-12-01T06:20:08.360151Z","shell.execute_reply":"2023-12-01T06:20:08.374354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the dataset and apply preprocessing","metadata":{}},{"cell_type":"code","source":"# split dataset into training, validation and testing portions\ntrain = df['filename'].loc[(df['fold'] == '1') | (df['fold'] == '2') | (df['fold'] == '3')| (df['fold'] == '4')].reset_index(drop=True) # takes folds 1, 2, 3 and 4 for training\nval = df['filename'].loc[df['fold'] == '5'].reset_index(drop=True) # takes fold 5 for validation\ntest = df['filename'].loc[(df['fold'] == 'test')].reset_index(drop=True) # held-out test set\n\nprint(f'The \"TRAIN\" set contains {train.shape[0]} samples.')\nprint(f'The \"VALIDATION\" set contains {val.shape[0]} samples.')\nprint(f'The \"TEST\" set contains {test.shape[0]} samples.')","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:08.376369Z","iopub.execute_input":"2023-12-01T06:20:08.377005Z","iopub.status.idle":"2023-12-01T06:20:08.392821Z","shell.execute_reply.started":"2023-12-01T06:20:08.376974Z","shell.execute_reply":"2023-12-01T06:20:08.391938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preprocess the train set\nX_train, y_train = preprocess(train)\n\n# preprocess the validation set\nX_val, y_val = preprocess(val)\n\n# preprocess the validation set\nX_test, y_test = preprocess(test)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:20:08.394032Z","iopub.execute_input":"2023-12-01T06:20:08.39484Z","iopub.status.idle":"2023-12-01T06:23:45.553518Z","shell.execute_reply.started":"2023-12-01T06:20:08.394805Z","shell.execute_reply":"2023-12-01T06:23:45.55187Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the shape of the output equals the expected shape of the spectrogram\nX_train[0].shape == EXP_INPUT_SHAPE","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:23:45.561063Z","iopub.execute_input":"2023-12-01T06:23:45.565814Z","iopub.status.idle":"2023-12-01T06:23:45.57774Z","shell.execute_reply.started":"2023-12-01T06:23:45.56576Z","shell.execute_reply":"2023-12-01T06:23:45.576148Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build CNN arcitecture and train model","metadata":{}},{"cell_type":"code","source":"# set a random seed for reproducability\ntf.keras.utils.set_random_seed(42)\n\n\n# define the model architecture\nmodel = Sequential()\nmodel.add(Conv2D(16, (3, 3), activation='relu', input_shape=(128, 107,1)))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(SpatialDropout2D(0.5))\n\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\nmodel.add(SpatialDropout2D(0.5))\n\nmodel.add(Conv2D(64, (3,3), activation='relu'))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\nmodel.add(SpatialDropout2D(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.7))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# compile model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='BinaryCrossentropy', metrics=[tf.keras.metrics.AUC(curve='PR', name='PR-AUC')])\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:24:03.20025Z","iopub.execute_input":"2023-12-01T06:24:03.200814Z","iopub.status.idle":"2023-12-01T06:24:08.871417Z","shell.execute_reply.started":"2023-12-01T06:24:03.200785Z","shell.execute_reply":"2023-12-01T06:24:08.870175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 32\nhist = model.fit(x=X_train, \n                 y=y_train, \n                 epochs=100, \n                 validation_data=(X_val, y_val), \n                 class_weight={0: 3, 1:1},\n                 verbose=1,\n                 batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:24:08.874817Z","iopub.execute_input":"2023-12-01T06:24:08.876074Z","iopub.status.idle":"2023-12-01T06:28:33.165729Z","shell.execute_reply.started":"2023-12-01T06:24:08.876031Z","shell.execute_reply":"2023-12-01T06:28:33.164752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title('Loss')\nplt.plot(hist.history['loss'], 'r')\nplt.plot(hist.history['val_loss'], 'b')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:33.167277Z","iopub.execute_input":"2023-12-01T06:28:33.16759Z","iopub.status.idle":"2023-12-01T06:28:33.459165Z","shell.execute_reply.started":"2023-12-01T06:28:33.167562Z","shell.execute_reply":"2023-12-01T06:28:33.458254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.title('PR-AUC')\nplt.plot(hist.history['PR-AUC'], 'r')\nplt.plot(hist.history['val_PR-AUC'], 'b')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:33.460424Z","iopub.execute_input":"2023-12-01T06:28:33.460711Z","iopub.status.idle":"2023-12-01T06:28:33.674203Z","shell.execute_reply.started":"2023-12-01T06:28:33.460685Z","shell.execute_reply":"2023-12-01T06:28:33.67336Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate model performance against the held-out \"test\" set","metadata":{}},{"cell_type":"code","source":"y_prob = model.predict(X_test, batch_size=BATCH_SIZE)\nap_score = average_precision_score(y_true=y_test, y_score=y_prob)\nprint('Average precision on TEST set:', ap_score)\n\nPrecisionRecallDisplay.from_predictions(y_test, y_prob)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:33.678506Z","iopub.execute_input":"2023-12-01T06:28:33.678864Z","iopub.status.idle":"2023-12-01T06:28:34.537235Z","shell.execute_reply.started":"2023-12-01T06:28:33.678838Z","shell.execute_reply":"2023-12-01T06:28:34.536365Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the model","metadata":{}},{"cell_type":"code","source":"model_path = os.path.join('/kaggle/working/', 'cnn', 'model')\n\nif not os.path.exists(model_path):\n    os.makedirs(model_path)\n\nmodel.save(model_path)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:11:21.341673Z","iopub.execute_input":"2023-11-30T01:11:21.341959Z","iopub.status.idle":"2023-11-30T01:11:23.244467Z","shell.execute_reply.started":"2023-11-30T01:11:21.341933Z","shell.execute_reply":"2023-11-30T01:11:23.243553Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate model performance on continuous \"Environmental\" audio","metadata":{}},{"cell_type":"code","source":"# define a function to build a filepath from an environment hour number\ndef get_env_path(env_n):\n    filepath = os.path.join(ENV_DIR, f'{env_n}_AUDIO.wav')\n    return filepath","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:34.540607Z","iopub.execute_input":"2023-12-01T06:28:34.541619Z","iopub.status.idle":"2023-12-01T06:28:34.545708Z","shell.execute_reply.started":"2023-12-01T06:28:34.541592Z","shell.execute_reply":"2023-12-01T06:28:34.544908Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to retreive the class mappings for a particular env hour\ndef get_env_mappings(env_n):\n    index_n = str(env_n - 1)\n    df = pd.read_csv(os.path.join(DATA_DIR, 'environment_class_mappings.csv'))\n    \n    df = df[index_n]\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:34.547061Z","iopub.execute_input":"2023-12-01T06:28:34.547641Z","iopub.status.idle":"2023-12-01T06:28:34.558257Z","shell.execute_reply.started":"2023-12-01T06:28:34.547607Z","shell.execute_reply":"2023-12-01T06:28:34.557374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_env(env_n: None | int = None):\n    # if env_n == None, it is assumed all environmental audio clips will be processed together\n    if env_n == None:\n        bottom = 1\n        top = 7\n    else:\n        bottom = env_n\n        top = env_n + 1\n        \n    data = {'feature': [], 'label': []}\n    \n    for env in range(bottom, top):\n        path = get_env_path(env_n=env)\n        mappings = get_env_mappings(env_n=env)\n        \n        signal, sr = librosa.load(path)\n        \n        if sr != SR:\n            raise ValueError('Sample rate mismatch between audio and target')\n            \n        clip_segments = math.ceil(len(signal) / SAMPLES_PER_SEGMENT)\n    \n        specs = []\n        \n        for segment in range(clip_segments):\n            \n            start = SAMPLES_PER_SEGMENT * segment\n            end = start + SAMPLES_PER_SEGMENT - HOP_LENGTH\n            \n            fclass = mappings.iloc[segment]\n            \n            if env_n == None:\n                \n            \n                if fclass == 'ignore':\n\n                    continue\n                    \n            if fclass == 'ignore':\n                fclass = 0\n            \n            spec = librosa.feature.melspectrogram(y=signal[start:end], \n                                              sr=sr, n_fft=N_FFT, \n                                              n_mels=N_MELS, \n                                              hop_length=HOP_LENGTH,\n                                              window='hann')\n            \n            db_spec = librosa.power_to_db(spec, ref=0.0)\n            \n            db_spec = normalise_array(db_spec)\n            \n            if db_spec.shape[1] == EXP_VECTORS_PER_SEGMENT:\n                \n                data['feature'].append(db_spec)\n                data['label'].append(fclass)\n            \n            elif db_spec.shape[1] < EXP_VECTORS_PER_SEGMENT:\n                n_short = EXP_VECTORS_PER_SEGMENT - db_spec.shape[1]\n                db_spec = np.pad(db_spec, [(0, 0), (0, n_short)], 'constant')\n                data['feature'].append(db_spec)\n                data['label'].append(int(fclass))\n                \n                \n            \n    # check the arrays are of the same length\n    if len(data['feature']) == len(data['label']):\n        return data\n    \n    else:\n        print('array mismatch - check output')\n        return data","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:34.559665Z","iopub.execute_input":"2023-12-01T06:28:34.559969Z","iopub.status.idle":"2023-12-01T06:28:34.572332Z","shell.execute_reply.started":"2023-12-01T06:28:34.559936Z","shell.execute_reply":"2023-12-01T06:28:34.571592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preprocess the environmental audio set\nenv_set = preprocess_env()\nX_env = np.asarray(env_set['feature'])\ny_env = np.asarray(env_set['label']).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:28:34.573242Z","iopub.execute_input":"2023-12-01T06:28:34.5735Z","iopub.status.idle":"2023-12-01T06:29:46.155906Z","shell.execute_reply.started":"2023-12-01T06:28:34.573477Z","shell.execute_reply":"2023-12-01T06:29:46.154445Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluate model performance on the env set\nmodel.evaluate(x=X_env, y=y_env, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:29:46.163431Z","iopub.execute_input":"2023-12-01T06:29:46.167314Z","iopub.status.idle":"2023-12-01T06:29:47.316239Z","shell.execute_reply.started":"2023-12-01T06:29:46.167259Z","shell.execute_reply":"2023-12-01T06:29:47.315319Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_prob = model.predict(X_env, batch_size=BATCH_SIZE)\nap_score = average_precision_score(y_true=y_env, y_score=y_prob)\nprint(ap_score)\n\nPrecisionRecallDisplay.from_predictions(y_env, y_prob)","metadata":{"execution":{"iopub.status.busy":"2023-12-01T06:29:47.317334Z","iopub.execute_input":"2023-12-01T06:29:47.317641Z","iopub.status.idle":"2023-12-01T06:29:48.501924Z","shell.execute_reply.started":"2023-12-01T06:29:47.317615Z","shell.execute_reply":"2023-12-01T06:29:48.501027Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot environmental predictions against the \"ground truth\"","metadata":{}},{"cell_type":"code","source":"# function to fetch the indices to ignore (periods which transition between positive and negative classes)\ndef fetch_ignore_indices(env_n):\n    \n    df = pd.read_csv(os.path.join(DATA_DIR, 'environment_class_mappings.csv'))\n    \n    arr = df[str(env_n-1)]\n    \n    ignore_list = []\n    for i in arr.index:\n        if arr.iloc[i] == 'ignore':\n            ignore_list.append(i)\n        else:\n            pass\n\n    lst = []\n    fst = []\n    for i in range(0, len(ignore_list)):\n\n        if i == 0:\n            first = ignore_list[i]\n            fst.append(first)\n\n        elif i == len(ignore_list)-1:\n            last = ignore_list[-1]\n            lst.append(last)\n\n        else:\n            if (ignore_list[i]-1 == ignore_list[i-1]) & (ignore_list[i]+1 == ignore_list[i+1]):\n                pass\n\n            elif (ignore_list[i]-1 == ignore_list[i-1]) & (ignore_list[i]+1 != ignore_list[i+1]):\n                last = ignore_list[i]\n                lst.append(last)\n\n            elif (ignore_list[i]-1 != ignore_list[i-1]) & (ignore_list[i]+1 == ignore_list[i+1]):\n                first = ignore_list[i]\n                fst.append(first)\n    \n    \n    if len(fst) == len(lst):\n        \n        tupes = []\n\n        for i in range(0, len(fst)):\n            tupes.append((fst[i], lst[i]+1))\n    \n    else:\n        print('array length mismatch')\n    \n    return tupes","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:12:36.352561Z","iopub.execute_input":"2023-11-30T01:12:36.352929Z","iopub.status.idle":"2023-11-30T01:12:36.365573Z","shell.execute_reply.started":"2023-11-30T01:12:36.352894Z","shell.execute_reply":"2023-11-30T01:12:36.364627Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function to predict across an environmental hour, then plot the predictions against the ground truth\ndef plot_env_predictions(model_name: str, env_n: int):\n    \n    title = f'ENV_{str(env_n)}_{model_name}'\n    \n    model_path = f'/kaggle/working/{model_name}/model'\n    \n    env_set = preprocess_env(env_n = env_n)\n    \n    X = np.asarray(env_set['feature'])\n    y = np.asarray(env_set['label']).astype(int)\n    \n    model = tf.keras.models.load_model(model_path)\n    \n    y_prob = model.predict(X, batch_size=32, verbose=0)\n    \n    df = pd.read_csv('/kaggle/input/audio-dataset-of-low-flying-aircraft-aerosonicdb/environment_mappings_raw.csv')\n    idx_n = str(int(env_n)-1)\n    \n    raw = df[idx_n]\n    \n    dfn = pd.read_csv(os.path.join(DATA_DIR, 'environment_class_mappings.csv'))\n    ig = dfn[idx_n]\n    ig = ig.replace('1', '0')\n    ig = ig.replace('ignore', '1')\n    \n    tot_frames = len(y_prob)\n    \n    start = 0  \n    end = start + tot_frames\n    \n    fig = plt.figure(figsize=(12, 4))\n    ax = plt.subplot(111)\n    \n    # fetch the transition areas\n    ig_idx = fetch_ignore_indices(env_n=env_n)\n    # mark the transition areas\n    for tupe in ig_idx:\n        if (tupe[0] >= start) & (tupe[1] <= end):\n            ax.axvspan(tupe[0], tupe[1], facecolor='gold', alpha=0.25)\n    \n    # set tick markers and labels\n    x_ticks = np.arange(start, (end+1), step=60)\n    x_labels = np.arange(start//12, ((end//12)+1), step=5)\n    plt.xticks(x_ticks, x_labels)\n        \n    gt = ax.plot(range(len(raw[start:end])), raw[start:end], linewidth=1.5, linestyle='dotted', color='seagreen', label='Ground truth')\n    thresh = ax.axhline(y=0.5, xmin=-0.05, xmax=1, color='r', linewidth=0.7, alpha=0.5, label='50% threshold')\n    pred = ax.plot(range(len(y[start:end])), y_prob[start:end], color='k', linewidth=.25, alpha=0.9)\n    ax.fill_between(x=range(len(y[start:end])), y1=y_prob[start:end].flatten(), alpha=0.1, color='k')\n    plt.ylim(-0.01,1.1)\n    plt.xlabel('Time (minutes)')\n    plt.ylabel('Probability')\n    patch = Patch(facecolor='gold', alpha=0.25, label='Onset/Outset (ignore)')\n    patch2 = Patch(facecolor='k', alpha=0.1, edgecolor=(0.0, 0.0, 0.0, 0.9), label='Model prediction')\n    plt.title(f'Model: {model_name}, Env: {env_n}')\n\n    # Shrink current axis's height by 10% on the bottom\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0 + box.height * 0.25,\n                     box.width, box.height * 0.75])\n\n    # Put a legend below current axis\n    ax.legend(handles=[gt[0], thresh, patch2, patch], loc='upper center', bbox_to_anchor=(0.5, -0.25),\n              fancybox=True, shadow=False, ncol=4, prop={'size': 8})\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:12:36.366734Z","iopub.execute_input":"2023-11-30T01:12:36.367009Z","iopub.status.idle":"2023-11-30T01:12:36.38545Z","shell.execute_reply.started":"2023-11-30T01:12:36.366985Z","shell.execute_reply":"2023-11-30T01:12:36.384747Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_env_predictions('cnn', env_n=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:12:36.386544Z","iopub.execute_input":"2023-11-30T01:12:36.386951Z","iopub.status.idle":"2023-11-30T01:12:49.117264Z","shell.execute_reply.started":"2023-11-30T01:12:36.38691Z","shell.execute_reply":"2023-11-30T01:12:49.116195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_env_predictions('cnn', env_n=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:12:49.118812Z","iopub.execute_input":"2023-11-30T01:12:49.119228Z","iopub.status.idle":"2023-11-30T01:13:01.917717Z","shell.execute_reply.started":"2023-11-30T01:12:49.119191Z","shell.execute_reply":"2023-11-30T01:13:01.916743Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_env_predictions('cnn', env_n=3)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:13:01.919161Z","iopub.execute_input":"2023-11-30T01:13:01.91948Z","iopub.status.idle":"2023-11-30T01:13:14.428261Z","shell.execute_reply.started":"2023-11-30T01:13:01.919453Z","shell.execute_reply":"2023-11-30T01:13:14.427169Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_env_predictions('cnn', env_n=4)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:13:14.434486Z","iopub.execute_input":"2023-11-30T01:13:14.434821Z","iopub.status.idle":"2023-11-30T01:13:26.585481Z","shell.execute_reply.started":"2023-11-30T01:13:14.434795Z","shell.execute_reply":"2023-11-30T01:13:26.584441Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_env_predictions('cnn', env_n=5)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:13:26.586822Z","iopub.execute_input":"2023-11-30T01:13:26.587176Z","iopub.status.idle":"2023-11-30T01:13:39.204319Z","shell.execute_reply.started":"2023-11-30T01:13:26.587148Z","shell.execute_reply":"2023-11-30T01:13:39.203324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_env_predictions('cnn', env_n=6)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T01:13:39.205665Z","iopub.execute_input":"2023-11-30T01:13:39.206367Z","iopub.status.idle":"2023-11-30T01:13:52.092618Z","shell.execute_reply.started":"2023-11-30T01:13:39.206331Z","shell.execute_reply":"2023-11-30T01:13:52.091642Z"},"trusted":true},"outputs":[],"execution_count":null}]}